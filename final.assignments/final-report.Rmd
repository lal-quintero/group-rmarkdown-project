---
title: "Uisge Beatha: Water of Life or Watered Down? Applied Multivariate Methods in Scotch Differentiation"
author: "Levi Quintero, Kirill Galiev, Jiaxi Sun, Zion Swinburne"
date: ""
output: 
  bookdown::pdf_document2:
    latex_engine: xelatex
bibliography: ["ref.bib"]
biblio-style: "apalike"
link-citations: true
header-includes:
    - \usepackage{bbm}
    - \usepackage{float}
    - \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(300568257, kind="Mersenne-Twister")

library(patchwork)
library(ggplot2)
library(ggpubr)
library(GGally)
library(ggbiplot)
library(ggcorrplot)
library(extrafont)
library(matlib)
library(bookdown)
library(RColorBrewer)
library(ggpubr)
library(ggthemes)
  theme_set(theme_pander())
library(extrafont)
  library(knitr)
library(kableExtra)
library(gridExtra)
library(grid)
library(dplyr)
library(reshape2)
library(Hotelling)
  library(mvtnorm)
  library(ggExtra)
  library(cluster)
  library(tibble)
  
  # Create the whisky data as a dataframe
whisky_data <- data.frame(
  Sample_no = c("1", "2a", "3a", "4a", "5a", "6a", "7a", "8a", "9a", "10a", "11a", "12a", "13a", "14a", "15a", "16", "17", "18a", "19", "20a", "21", "22a", "23a", "24", "25", "26", "27", "28", "29", "30", "31", "32"),

  Descriptor = c("Blend", "Blend", "Blend", "Blend", "Blend", "Blend", "Blend", "Blend", "Counterfeit", "Counterfeit", "Counterfeit", "Counterfeit", "Counterfeit", "Grain", "Grain", "Highland", "Highland", "Island", "Island", "Island", "Island", "Lowland", "Lowland", "Speyside", "Speyside", "Speyside", "Speyside", "Speyside", "Speyside", "Speyside", "Speyside", "Speyside"),

  Distillery = c("Baile Nicol Jarvie", "Bells", "Chivas", "Dewars", "Johnnie Walker", "The Famous Grouse", "Whyte and Mackay", "William Grant", "Unknown 1", "Unknown 2", "Unknown 3", "Unknown 4", "Unknown 5", "Grain matured", "Grain unmatured", "Glengoyne", "Glenmorangie", "Bowmore", "Bruichladdie", "Bunnahabhain", "Talisker", "Auchentoshan", "Glenkinchie", "Balvenie", "Craigellachie", "Dufftown", "Glen Elgin", "Glenburgie", "Glennfiddich", "Glenrothes", "Knockando", "Linkwood"),

  P = c(0.152, 0.653, 0.375, 0.121, 0.326, 0.145, 0.067, 0.239, 0.089, 0.088, 0.279, 0.320, 0.120, 0.034, 0.084, 1.04, 0.126, 0.914, 1.63, 2.24, 0.034, 0.169, 0.108, 0.695, 0.096, 0.883, 0.115, 2.00, 0.317, 0.953, 0.051, 0.276),

  S = c(1.10, 1.58, 0.809, 1.16, 1.09, 0.615, 0.576, 0.748, 4.06, 14.7, 15.9, 22.1, 26.1, 2.23, 5.53, 5.57, 0.796, 6.67, 5.48, 7.54, 4.85, 1.46, 2.45, 3.85, 0.819, 4.64, 1.35, 7.91, 2.72, 4.11, 1.03, 1.05),

  Cl = c(0.173, 0.238, 0.193, 0.157, 0.180, 0.097, 0.151, 0.147, 0.066, 0.072, 0.083, 0.596, 0.071, 0.252, 0.113, 0.343, 0.245, 0.316, 0.697, 1.35, 0.362, 0.417, 0.176, 0.120, 0.177, 0.130, 0.404, 0.185, 0.344, 0.399, 0.191, 0.207),

  K = c(7.86, 4.93, 4.31, 3.20, 5.48, 2.74, 2.36, 2.84, 0.336, 1.23, 0.811, 2.32, 2.37, 6.44, 3.25, 24.2, 6.95, 21.1, 36.5, 36.2, 5.67, 11.7, 7.76, 20.3, 6.11, 14.0, 9.27, 37.7, 12.4, 16.7, 5.14, 6.22),

  Ca = c(1.45, 1.40, 1.22, 1.14, 0.526, 0.416, 0.745, 0.976, 1.24, 1.40, 1.36, 1.78, 1.63, 1.04, 1.35, 0.857, 0.859, 0.868, 4.13, 2.12, 0.607, 0.681, 0.738, 0.765, 0.633, 1.05, 1.40, 1.65, 0.660, 1.83, 0.605, 1.01),

  Mn = c(0.032, 0.019, 0.019, 0.011, 0.018, 0.009, 0.012, 0.010, 0.007, 0.006, 0.006, 0.008, 0.010, 0.013, 0.012, 0.023, 0.035, 0.037, 0.038, 0.051, 0.018, 0.042, 0.031, 0.031, 0.024, 0.030, 0.031, 0.053, 0.029, 0.041, 0.017, 0.020),

  Fe = c(0.027, 0.110, 0.044, 0.050, 0.103, 0.050, 0.047, 0.021, 0.154, 0.025, 0.057, 0.019, 0.082, 0.115, 0.076, 0.197, 0.025, 0.148, 0.288, 0.184, 0.070, 0.128, 0.106, 0.121, 0.094, 0.078, 0.046, 0.134, 0.132, 0.137, 0.094, 0.064),

  Cu = c(0.186, 0.242, 0.196, 0.189, 0.286, 0.208, 0.159, 0.137, 0.085, 0.052, 0.038, 0.038, 0.187, 0.174, 0.164, 1.251, 0.523, 0.548, 0.587, 0.580, 0.277, 1.32, 0.434, 0.380, 0.239, 0.533, 0.195, 0.198, 0.519, 1.030, 0.432, 0.769),

  Zn = c(0.015, 0.021, 0.007, 0.018, 0.020, 0.007, 0.019, 0.020, 0.038, 0.018, 0.016, 0.015, 0.194, 0.019, 0.046, 0.041, 0.011, 0.032, 0.066, 0.057, 0.033, 0.037, 0.022, 0.035, 0.025, 0.024, 0.029, 0.043, 0.193, 0.029, 0.020, 0.019),

  Br = c(0.002, 0.005, 0.003, 0.003, 0.002, 0.002, 0.003, 0.003, 0.005, 0.004, 0.002, 0.068, 0.012, 0.004, 0.010, 0.004, 0.003, 0.007, 0.034, 0.014, 0.003, 0.006, 0.002, 0.005, 0.005, 0.002, 0.006, 0.008, 0.004, 0.007, 0.008, 0.004),

  Rb = c(0.006, 0.003, 0.002, 0.003, 0.002, 0.001, 0.002, 0.002, 0.001, 0.001, 0.002, 0.001, 0.005, 0.006, 0.003, 0.016, 0.006, 0.018, 0.039, 0.037, 0.006, 0.012, 0.007, 0.024, 0.006, 0.014, 0.009, 0.026, 0.013, 0.014, 0.005, 0.006)
)


#set as factor
whisky_data$Sample_no <- as.factor(whisky_data$Sample_no)
whisky_data$Descriptor <- as.factor(whisky_data$Descriptor)
whisky_data$Distillery <- as.factor(whisky_data$Distillery)



#subset data for handling
noclass_whisky <- subset(whisky_data, select = -c(Sample_no, Descriptor, Distillery))

class_whisky <- subset(whisky_data, select = -c(Sample_no, Distillery))



#logged as in paper
logged.c.whisky <- class_whisky
logged.c.whisky[,-1] <- log(class_whisky[,-1])


#scaled as in paper
scale.log.w <- logged.c.whisky
scale.log.w[,-1] <- scale(logged.c.whisky[,-1])


#melt for boxplots
whiskyclass.melt <- melt(data= class_whisky,
                         measure.vars = 2:12,
                         variable.name = "Variable",
                         value.name = "Value",
                         id.vars = 1)

```


# Introduction
jess

Scotch whisky represents one of the United Kingdom’s most valuable export
commodities, contributing approximately £3.95 billion to the economy in 2015, or
about 25% of total food and drink exports. Its global popularity, particularly for
blended whiskies—which can consist of 60–80% grain whisky—makes the industry
highly vulnerable to counterfeiting(@shand2017multivariate; @storrie1962scotch; @bower2016scotch; @SWACWG2021). Counterfeit products not only undermine
economic revenue and brand integrity but also pose risks to consumer safety .
 
Traditional whisky authentication methods rely on techniques such as gas
chromatography, which profiles volatile organic congeners, and stable isotope ratio
analysis, which can differentiate production origins. While these methods are
scientifically robust, they require expensive instrumentation, laboratory-bound
environments, and trained specialists. (@shand2017multivariate) Consequently, they are not practical for rapid, field-based screening in supply chains, retail, or customs inspection.

An alternative approach lies in trace element analysis. Elemental signatures in
whisky derive from raw materials (such as water and grain), production equipment,
storage vessels, and potential additives. Previous research suggests that these
elemental profiles can provide a reliable chemical “fingerprint” to detect fraudulent
products and explore provenance (@power2020brief).

Building on this idea, the present study applies Total Reflection X-Ray Fluorescence
(TXRF) spectroscopy in combination with multivariate statistical analysis to evaluate
whether elemental concentration data can reliably classify whisky samples.
Specifically, we investigate whether TXRF measurements across 11 trace elements
(P, S, Cl, K, Ca, Mn, Fe, Cu, Zn, Br, and Rb) can:
- Differentiate authentic Scotch whiskies from counterfeit products,
- Distinguish between blended/grain whiskies, and
- Explore whether regional provenance (Highland, Island, Lowland, Speyside)
leaves a measurable elemental signature.

The statistical workflow includes data preprocessing (log transformation and
Mahalanobis distance assessment), dimensionality reduction by Principal
Component Analysis (PCA), classification by Linear Discriminant Analysis (LDA),
and clustering using Partitioning Around Medoids (PAM) and hierarchical methods.

Robustness of classification is further tested across distance measures (Euclidean, Manhattan and correlation).


## Our Aims

Reassesing/Using data already collected is both cost-saving and effective within acadamia when resources are limited. We used  @shand2017multivariate XTRF collected trace chem sampels from 7 whisky types. We did so to assess reproducability as well further applications of multivariaite analytical methods to this method of chemical sampling. If successful and widely applicable, this presents a novel, robust and cost affordable way to differentiate counterfeits, grains and malt whiskies.

## Brief description Sharon et al., study

```{r chemdata, echo=FALSE, warning= FALSE}

whisky_data %>%
  knitr::kable(
    caption = "Whisky Origin and Chemical Data",
    digits = 3,
    booktabs = TRUE
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("scale_down", "HOLD_position"),
    font_size = 9
  ) %>%
  kableExtra::footnote(
    general = "Chemical concentrations reported in mg per L. All samples analyzed using total reflection X-ray fluorescence (XTRF). Derived from Shand et al. 2017.",
    general_title = "Note:",
    footnote_as_chunk = TRUE
  )
```

## Our hypothesis

As we wish to use all available data to retain maximum information, we wish to employ multiple common cluster analysis methods to see if can produce useful and agreeing groups for whisky differentiation. As @shand2017multivariate was able to differentiate counterfeits successfully from all other whiskies within a limited principal component (PC) space (PC1-PC3), we wished to attempt to replicate this using k-means, partitioning about mediods (PAM), and agglomerative hierarchical clustering to assess if XTRF is a robust method for counterfeit detection. 

Further, we wish to see if the data presents us with any compelling groups other than our pre-applied ones, we will aim at assessing data structure and seeing if groups emerging from cluster analysis agree.


# EDA

## Exploratory Data Analysis (EDA)



All data analysis were conducted in R version 4.4.3 [@R-base].  Summary statistics for whisky sample chemical trace TXRF data (Table \@ref(tab:chemdata)) show large differences in range and variability between trace chemical variables magnitude and variability (Table \@ref(tab:summary)). Variables show differences in the range of the magnitude $10^4$, with Rb displaying a range of 0.030 and K displaying a range of 36.964. When density plots of by-chemical observations were plotted, all displayed a strong right skew. Further, whisky class observations (n=32) were highly unbalanced (Grain = 2, Highland = 2, Lowland = 2, Island = 4, Counterfeit = 5, Blend = 8, Speyside = 9) across the observations of these 11  chemical variables (P, S, Cl, K, Ca, Mn, Fe, Cu, Zn, Br, Rb).

```{r summary, echo =FALSE, warning=FALSE}

whisky_summary <- whisky_data %>%
  dplyr::summarise(
    dplyr::across(
      .cols = where(is.numeric),
      .fns = list(
        Mean = ~mean(.x, na.rm = TRUE),
        Median = ~median(.x, na.rm = TRUE),
        SD   = ~sd(.x, na.rm = TRUE),
        Min  = ~min(.x, na.rm = TRUE),
        Max  = ~max(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    )
  ) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = c("Variable", ".value"),
    names_sep = "_"
  )

# print summary table
whisky_summary %>%
  knitr::kable(
    caption = "Summary Statistics for Whisky Chemical Variables",
    digits = 3,
    booktabs = TRUE
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("scale_down", "HOLD_position"),
    font_size = 9
  ) %>%
  kableExtra::footnote(
    general = "Values represent mean, median, standard deviation (S.D.), and range (min–max) for each chemical element measured across all whisky samples.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )

```



We initially assessed if the TXRF data derived from @shand2017multivariate (Table \@ref(tab:chemdata)) followed a multivariate normal distribution ($X \sim N_{11}(\mu, \Sigma)$) via visual comparisons of observation Mahalanobis distances ($d^2_M(X, \mu)$) to their expected quantiles and a QQ plot line (\@ref(fig:mah)), observation density plots by factor, as well as by conducting a Henze-Zirkler Test of Multivariate normality ($HZ = 1.325, p < 0.001$; Table \@ref(tab:mnorm)). As the data clearly did not conform to $X \sim N_{11}(\mu, \Sigma)$ and exhibited starkly different magnitudes in observations, we log transformed the observations and re-aplied the same analysis (Fig. \@ref(fig:mah); Table \@ref(tab:mnorm)). Results now conformed to $X \sim N_{11}(\mu, \Sigma)$ with no outliers ($d_M > 0.99$), and displayed distributions generally centered about 0. Anderson-Darling tests of univariate normality confirmed that 9 of the 11 chemical variables displayed normality while Zn ($A^2 = 0.872, p = 0.022$) and Br ($A^2 = 1.075, p=0.007$) displayed significant departures. This log-transformed data was henceforth used in this analysis.


```{r mah, fig.cap="Mahalanobis histogram of multivariate density distribution of whisky observations with cutoff points marked at 0.90,0.95 and 0.99 density quantiles, overlaid with a chi-squared distribution kernel with 11 degrees of freedom. ", fig.align='center', fig.pos="H", echo=FALSE, warning=FALSE}

mu.hat.w <- colMeans(noclass_whisky)


sigma.hat.w <- cov(noclass_whisky)


dMw <- mahalanobis(noclass_whisky, center=mu.hat.w, cov=sigma.hat.w)


upper.quantiles.w <- qchisq(c(.9, .95, .99), df=11)
density.at.quantiles.w <- dchisq(x=upper.quantiles.w, df=11)
cut.points.w <- data.frame(upper.quantiles.w, density.at.quantiles.w)


p.dm <- ggplot(data.frame(dMw), aes(x = dMw)) +
  geom_histogram(aes(y = after_stat(density)), bins = nclass.FD(dMw),
                 fill = "white", col = "black") +
  geom_rug() +
  stat_function(fun = dchisq, args = list(df = 11),
                col = "firebrick", size = 1.5, alpha = .7, xlim = c(0, 25)) +
  geom_segment(data = cut.points.w,
               aes(x = upper.quantiles.w, xend = upper.quantiles.w,
                   y = rep(0, 3), yend = density.at.quantiles.w),
               col = "navyblue", size = 2) +
  labs(
    title = "a)",
    x = NULL,
    y = "Histogram and density"
  ) +
  theme(plot.title = element_text(hjust = 0))



######################

nc_whisky.log <- log(noclass_whisky)

mu.hat.l <- colMeans(nc_whisky.log)


sigma.hat.l <- cov(nc_whisky.log)


l.dMw <- mahalanobis(nc_whisky.log, center=mu.hat.l, cov=sigma.hat.l)


upper.quantiles.w <- qchisq(c(.9, .95, .99), df=11)
density.at.quantiles.w <- dchisq(x=upper.quantiles.w, df=11)
cut.points.w <- data.frame(upper.quantiles.w, density.at.quantiles.w)


mah.plot2 <- ggplot(data.frame(l.dMw), aes(x=l.dMw)) +
  geom_histogram(aes(y=after_stat(density)), bins=nclass.FD(l.dMw),
                 fill="white", col="black") +
  geom_rug() +
  stat_function(fun=dchisq, args = list(df=11),
                col="firebrick", size=1.5, alpha=.7, xlim=c(0,25)) +
  geom_segment(data=cut.points.w,
               aes(x=upper.quantiles.w, xend=upper.quantiles.w,
                   y=rep(0,3), yend=density.at.quantiles.w),
               col="navy", size=2) +
labs(
    title = "b)",
    x = "Mahalanobis distances and cut points",
    y = NULL
  ) +
  theme(plot.title = element_text(hjust = 0))

grid.arrange(p.dm, mah.plot2, ncol = 1)



```





```{r tablesetup, echo=FALSE, warning=FALSE, results='hide'}
noclass_whisky2 <- noclass_whisky

#more detailed surprise observation
noclass_whisky2$surprise_detailed <- cut(dMw,
                                         breaks = c(0,
                                                    qchisq(0.5, df=ncol(noclass_whisky)),   # 50%
                                                    qchisq(0.75, df=ncol(noclass_whisky)),  # 75%
                                                    qchisq(0.9, df=ncol(noclass_whisky)),   # 90%
                                                    qchisq(0.95, df=ncol(noclass_whisky)),  # 95%
                                                    qchisq(0.99, df=ncol(noclass_whisky)),  # 99%
                                                    Inf),
                                         labels = c("Bottom_50%", "50-75%", "75-90%",

                                                    "90-95%", "95-99%", "Top_1%"))





#more detailed look at suprising observations


surprise_summary.w <- table(noclass_whisky2$surprise_detailed)

surprise_df.w <- data.frame(
  Category = names(surprise_summary.w),
  Count = as.numeric(surprise_summary.w),
  Percentage = round(100 * as.numeric(surprise_summary.w) / sum(surprise_summary.w), 1)
)

test_stat <- 1.325
p_val <- "<0.001"

surprise_df2 <- rbind(
  surprise_df.w,
  data.frame(
    Category = "Henze-Zirkler Test",
    Count = test_stat,
    Percentage = p_val
  )
)





#more detailed surprise observation

nc_whisky.log2 <- nc_whisky.log

nc_whisky.log2$surprise_detailed <- cut(l.dMw,
                                        breaks = c(0,
                                                   qchisq(0.5, df=ncol(noclass_whisky)),   # 50%
                                                   qchisq(0.75, df=ncol(noclass_whisky)),  # 75%
                                                   qchisq(0.9, df=ncol(noclass_whisky)),   # 90%
                                                   qchisq(0.95, df=ncol(noclass_whisky)),  # 95%
                                                   qchisq(0.99, df=ncol(noclass_whisky)),  # 99%
                                                   Inf),
                                        labels = c("Bottom_50%", "50-75%", "75-90%",

                                                   "90-95%", "95-99%", "Top_1%"))

det.logsuprise <- table(nc_whisky.log2$surprise_detailed)

log.supr_df.2 <- data.frame(
  Category = names(det.logsuprise),
  Count = as.numeric(det.logsuprise),
  Percentage = round(100 * as.numeric(det.logsuprise) / sum(det.logsuprise), 1)
)




test_stat2 <-  0.984
p_val2 <- "0.137"

surprise_dflog <- rbind(
  log.supr_df.2,
  data.frame(
    Category = "Henze-Zirkler Test",
    Count = test_stat2,
    Percentage = p_val2
  )
)

#display

# Create kable versions with same styling
table1 <- surprise_df2 %>%
  knitr::kable(
    col.names = c("Distance Category", "Count/HZ", "%/P-val"),
    digits = 3,
    booktabs = TRUE,
    caption = NULL
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position"),
    font_size = 9
  ) %>%
  kableExtra::row_spec(nrow(surprise_df2), hline_after = TRUE)

table2 <- surprise_dflog %>%
  knitr::kable(
    col.names = c("Distance Category", "Count/HZ", "%/P-val"),
    digits = 3,
    booktabs = TRUE,
    caption = NULL
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position"),
    font_size = 9
  ) %>%
  kableExtra::row_spec(nrow(surprise_dflog), hline_after = TRUE)
```


```{r mnorm, echo=FALSE, warning=FALSE}




bind_rows(
  surprise_df2,
  surprise_dflog
) %>%
  knitr::kable(
    col.names = c("Distance Category", "Count/HZ", "%/P-val"),
    digits = 3,
    booktabs = TRUE,
    caption = "Summary of Surprising Observations by Data Transformation"
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position"),
    font_size = 9
  ) %>%
  kableExtra::pack_rows("Original Data", 1, 7) %>%
  kableExtra::pack_rows("Log-Transformed Data", 8, 14) %>%
  kableExtra::footnote(
    general = "Distance categories based on Mahalanobis distance quantiles. HZ denotes Henze-Zirkler test statistic for multivariate normality.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )
```


In @shand2017multivariate, an LDA analysis was conducted using the first 3 principal components. Due to highly imbalanced design of whisky class sampling, overall homogenity of whisky class covariance ($\Sigma_1 = \Sigma_2 = ... = \Sigma_7$) could not be established as the smallest classes (n = 2) are not $> p = 3$ and thus produced non-invertible covariance matrices. Thus, combined with the small class sizes likely making supervised LDA training unstable, we believe that this rules out the suitability of this method of discriminant analysis.

Due to @shand2017multivariate's inability to draw cohesive classification results at the regional level, we sought to allow the data as well as literature evidence of whisky type composition to intuitively guide our selection of the number of assigned clusters ($k^*$) in this analysis. We first viewed box-plots by variable class and the overall correlation structure of the chemical variables. Drawing upon these observations, differential median trends emerged (particularly for variables Mn, Cu and Rb) between single-origin and blends, grains and counterfeits (Fig. \@ref(fig:box)). We then isolated the chemical variable correlation structures of the largest classes available to visually assess and compared respective mean vector differences between these using Hotelling $T^2$ tests. 

```{r box, fig.cap="Panel boxplots of log-transformed measurements of observations (mg/L) faceted by chemical (P, S, Cl, K, Ca, Mn, Fe, Cu, Zn, Br, Rb), with observations grouped by whisky type (Blend, Counterfeit, Grain, Highland, Lowland, Speyside, Island).", fig.align='center', fig.pos="H", echo=FALSE, warning=FALSE, message = FALSE}

log.box <- ggplot(data = whiskyclass.melt, aes(x = Descriptor, y = Value)) +
  geom_boxplot(aes(fill = Variable), notch = TRUE) +
  scale_y_log10() +
  facet_wrap(~ Variable, scales = "free") +
  # scale_fill_brewer(palette = "Dark2") +
  theme_pubclean() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        legend.position = "none") +
  labs(x = "Chemical Elements", y = "Measured Values")



log.box
```


Counterfeits (Counterfeit, Speyside: $T^2$ = 7083, p = 0.009), Blends (Counterfeit, Blend: $T^2$ = 928,150, p = 0.009) and Speyside (Speyside, Blend: $T^2$ = 213.48, p = 0.026) whisky classes all displayed significant mean vector differences and visually differing correlation structures while Island and Speyside (Both single-region origin malted-barley whiskies) did not (Island, Speyside: $T^2$= 474, p = 0.042). Further, blended whiskies are composed primarily of grains (wheat or maize, 60-80%) with little malted-barley, while single origin whiskies are of only of malt character [@storrie1962scotch; @bower2016scotch; @kew2016chemical; @SWACWG2021]. 

BoxM tests for the homogeneity of variances was unable to be performed within our feature space as all  class $n < p$, but correlation plots indicated likely differences in covariance matrices. These differences were to be visually supported when observations were projected within three-dimensional principal component  (PC) space, and counterfeits appeared completely linearly separable from all other observations.

Thus we re-aggregated the sampled whiskies into three logical overarching predictive whisky classes: "Counterfeits" (n=5), "Grains & Blends" (a combination of grain and blended classes; n=10), and "Provenance" (all whiskies of a single-region origin; n=17). This reclassification is further supported as both grain type (barley vs. other) and region of production being shown to influence chemical composition of whiskies under other analytical methods, thus we should expect the regional fingerprint to be diluted within grain & blended whiskies [@kew2016chemical; @roullier2020influence].

We then reassessed correlation plots of chemical observations for each of these three classes, as well as mean vector differences between them. We found striking correlation structure differences between the classes (particularly between counterfeit and provenance whiskies) as well as significant mean vector differences (\@ref(fig:cor); \@ref(tab:hotel)).

```{r cor, fig.cap="Correlation plots of whisky trace elements:(a) Correlation structure between all observations, (b) Correlation structure between  whiskies of provenance, (c) corrrelation structure between blended whiskies, (d)correlation structure between counterfeit whiskies", echo=FALSE, fig.width=14, fig.height=12, dpi=300, warning =FALSE}

logged.w.plot <- logged.c.whisky

logged.w.plot$Provenance <- ifelse(logged.c.whisky$Descriptor %in% c("Blend", "Counterfeit", "Grain"),
                                   as.character(logged.c.whisky$Descriptor),
                                   "Provenance")
logged.w.plot$Provenance <- ifelse(logged.w.plot$Provenance %in% c("Grain", "Blend"),
                                   "Grain_Blend",
                                   as.character(logged.w.plot$Provenance))

logged.w.plot$Provenance<- as.factor(logged.w.plot$Provenance)




corall <- ggcorrplot(cor(log(class_whisky[,-(1)])),
           method = "square",
           lab=TRUE,
           ggtheme = theme_tufte,
           colors = c("cyan", "white", "coral"),
           hc.order = TRUE,
           type = "lower")+
  theme(legend.position = "none")

provenance_data <- subset(logged.w.plot, logged.w.plot[,13] == "Provenance")

p.cor <- ggcorrplot(cor(provenance_data[, -c(1, 13)]),
           method = "square",
           lab=TRUE,
           ggtheme = theme_tufte,
           colors = c("cyan", "white", "coral"),
           hc.order = TRUE,
           type = "lower") +
  theme(legend.position = "none")



counterfeit_data <- subset(logged.w.plot, logged.w.plot[,13] == "Counterfeit")

c.cor <- ggcorrplot(cor(counterfeit_data[, -c(1, 13)]),
           method = "square",
           lab=TRUE,
           ggtheme = theme_tufte,
           colors = c("cyan", "white", "coral"),
           hc.order = TRUE,
           type = "lower")



blend_data <- subset(logged.w.plot, logged.w.plot[,13] == "Grain_Blend")

b.cor <- ggcorrplot(cor(blend_data[, -c(1, 13)]),
           method = "square",
           lab=TRUE,
           ggtheme = theme_tufte,
           colors = c("cyan", "white", "coral"),
           hc.order = TRUE,
           type = "lower")+
  theme(legend.position = "none")

library(ggcorrplot)
library(patchwork)

# 1. Shrink the numbers in the cells
corall <- corall + theme(text = element_text(size = 12), lab_size = 3) +
  theme(legend.position = "none")

p.cor <- p.cor + theme(text = element_text(size = 12), lab_size = 3) +
  theme(legend.position = "none")

b.cor <- b.cor + theme(text = element_text(size = 12), lab_size = 3) +
  theme(legend.position = "none")

c.cor <- c.cor + theme(text = element_text(size = 12), lab_size = 3)
# keep legend if desired

# 2. Combine plots 2x2 and add labels a,b,c,d
(corall + p.cor) / (b.cor + c.cor) +
  plot_annotation(tag_levels = "a") &
  theme(plot.tag.position = c(0,1),  # top-left
        plot.tag = element_text(size = 16, face = "bold"))




```

```{r hotel, echo=FALSE, warning=FALSE}

provcounter <- hotelling.test(subset(logged.w.plot, Provenance=="Provenance")[, -c(1, 13)],
                              subset(logged.w.plot, Provenance=="Counterfeit")[, -c(1, 13)])

provblend <- hotelling.test(subset(logged.w.plot, Provenance=="Provenance")[, -c(1, 13)],
                              subset(logged.w.plot, Provenance=="Grain_Blend")[, -c(1, 13)])

blendcounter <- hotelling.test(subset(logged.w.plot, Provenance=="Grain_Blend")[, -c(1, 13)],
                              subset(logged.w.plot, Provenance=="Counterfeit")[, -c(1, 13)])

test_results2 <- data.frame(
  Comparison = c("Provenance vs Counterfeit",
                 "Provenance vs Grain/Blend",
                 "Grain/Blend vs Counterfeit"),
  T2_Statistic = c(1181.9, 137.44, 474.57),
  P_Value = c(2.162e-07, 0.000272, 0.04191)
)

test_results2 %>%
  knitr::kable(
    caption = "Hotelling's T² Test Results",
    digits = 3,
    booktabs = TRUE
  ) %>%
  kableExtra::kable_styling(
    latex_options = c("scale_down", "HOLD_position"),
    font_size = 9
  ) %>%
  kableExtra::footnote(
    general = "Hotelling's T² tests were conducted on log-transformed whisky descriptors by the reaggregated classes providence, blends & grains, and counterfeits.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )
```

This led us to conduct Principle Component Analysis (PCA) to ascertain the magnitude and directionality of each chemical driver within a reduced space, as to infer and categorize chemical differences between groups later drawn from cluster analysis. Principal components analysis was conducted via using the prcomp() function on our scaled, log-transformed data. We returned to @shand2017multivariate's LDA procedure with our newly aggregated classes, and performed a global BoxM test to assess the equality of variance matrices for by group for PC1-PC3, which we found to be heterogeneous ($X^2_{12}, p = 0.041$) and thus discarded LDA as an analysis option.

We then implemented k-means, partitioning around medoids (PAM), and agglomerative hierarchical clustering to draw data-supported groups to test our predictive ones. In doing so, we scaled our log-transformed observations, except in the case of correlation distance hierarchical clustering which was conducted on the log-transformed data. Further, we wished to see if we could draw general consensus between these methods in support of using XTRF chemical composition sampling as a widely applicableand robust method to produce suitable data for general multivariate analysis to categorize whiskies by counterfeit, blend, or single origin status. 

## K-means Clustering

K-means analysis was conducted on the scaled-log transformed data using the kmeans() function with the default recommended Hartigan-Wong algorithm, iterations set to 100 (iter. max  = 100), and 50 random starts (nstart = 50) for all values of k. Setting random initializations to 50 reduces the chance of poor initial centroid allocation, while allowing 100 maximum iterations ensures convergence during the process of iterative group allocation of data points. 

This process was generated for $k = 1-10$, allowing $k^*$ to be chosen via visual assessment of total within sum of squares (WSS) reductions with concurrent silhouette plot analysis. 

## Partitioning Around Medoids (PAM) Clustering

PAM clustering was conducted using the pam() function from the cluster package with Euclidean distance [@kaufman_pam]. $k^*$ assessment was conducted via assessing silhouette plots and widths as well as via comparing clustering results to k-means clustering for respective $k^*$. 

## Agglomerative Hierarchical Clustering

All agglomerative hierarchical clustering was performed using hclust(), with the original clustering results of @shand2017multivariate reproduced via using euclidian distance with complete linkage. We attempted to find consistent hierarchical clustering results to these using other distances emphasizing absolute differences (Minkowski [a=3] and Chebyshev) and associated linkages (complete and Ward) as well as develop our own agglomerative hierarchical clustering models with better performance. Of those trialed, Euclidian (Ward linkage), Manhattan (complete and Ward linkage), and Correlation ($1-r$, Ward linkage) distances were retained for comparison and analysis.

## Quality Metrics 

After $k^*$ was established confusion matrices were produced for all clustering results and compared to our proposed groups (Provenance, blend/grain, and counterfeit whiskies) with global quality statistics ($Acc$, $F1\text{-score}_M$, $TNR_M$, $F1\text{-score}_{\mu}$, $PPV_{\mu}$, $TPR_{\mu}$) calculated. The best scoring clustering model was then assessed via class-wise quality metrics ($Acc_i$, $MR_i$, $PPV_i$, $TPR_i$, $TNR_i$, $F1\text{-score}_i$).

# Results

## PCA Analysis:

The first three principal components contained 79.16% of our datas variance within the PC feature space (S.D. - PC1 = 2.265, PC2 = 1.549, PC = 1.085; Variance explained - PC1 = 46.64%, PC2 = 21.82%, PC3 = 10.71%). Further principal components are associated with rapidly decreasing proportions of variance explained (\@ref(fig:elbow); \@ref(tab:var)). 

```{r elbow, fig.cap="Elbow plot of principal components (PC) and associated eigen values (variance).  The red line denotes where approximately 80% of the data's variance is contained (0.7916), with the remaining 20% associated with the red shading over PC 4-11", echo=FALSE, warning =FALSE}


PCA.whisky.log <- prcomp(scale.log.w[,-(1)], center = FALSE, scale = FALSE)


logged.w.plot$sample <- as.factor(whisky_data$Sample_no)

pc_scores <- data.frame(PCA.whisky.log$x[,1:3])

pc_scores$groups <- logged.w.plot$Descriptor  

pc_scores$groups2 <- logged.w.plot$Provenance  



logged.w.plot$sample <- as.factor(whisky_data$Sample_no)

pc_scores <- data.frame(PCA.whisky.log$x[,1:3])

pc_scores$groups <- logged.w.plot$Descriptor  

pc_scores$groups2 <- logged.w.plot$Provenance  

pca_var <- data.frame(
  PC = paste0("PC", 1:length(PCA.whisky.log$sdev)),
  Variance = PCA.whisky.log$sdev^2,
  PC_num = 1:length(PCA.whisky.log$sdev)
)

ggplot(pca_var, aes(x = PC_num, y = Variance)) +
  geom_line(size = 1.5) +
  geom_point(size = 3) +
  geom_vline(xintercept = 3, linetype = "dashed", color = "red", linewidth = 1) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 0.08163043, ymax = 1.178, alpha = 0.2, fill = "coral") +
    labs(x = "Principal Component",
       y = expression(Variance~Explained~(Associated~(lambda)))) +
  scale_x_continuous(breaks = 1:nrow(pca_var)) +
  theme_tufte() +
  theme(axis.title = element_text(size = 14),        
        axis.text = element_text(size = 12),
         axis.text.x = element_text(size = 12),    
        axis.text.y = element_text(size = 12))

```

```{r var, echo=FALSE, warning=FALSE}
log.whisky.summary <- summary(PCA.whisky.log)
log.whisky.summary$importance %>%
  as.data.frame() %>%
  kable(
    digits = 4,
    caption = "Standardized Log-data PCA Summary",
    col.names = paste0("PC", 1:ncol(.)),
    booktabs = TRUE,
    align = "c"
  ) %>%
  kable_styling(
    latex_options = c("hold_position", "scale_down"),  # auto-scale
    full_width = FALSE,
    position = "center"
  ) %>%
  footnote(
    general = "Standardized Principal Components of the scaled log-transformed whisky trace chemical observations and their associated standard deviation, with proportional variance explained.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )
```
PC1 variance was composed by all positive loadings primarily from P = 0.31, Cl = 0.315, K = 0.404, Mn = 0.379, Fe = 0.310, Cu = 0.327, Zn = 0.241, and Rb = 0.415 with minor contributions from Br,Ca and S (<0.20). The largest loadings contributing to PC2 are those contributing little to PC1 such as Br (0.454), Ca (0.475) and S (0.534), with others generally having negative or small contributions. PC3 further differentiates the PC space with varying large positive and negative inputs (Table \@ref(tab:loading)).

```{r loading, echo =FALSE, warning=FALSE}
pc.makeup <- data.frame(PCA.whisky.log$rotation[,1:3])

# Transpose so that PCs are rows and variables are columns
pc.makeup.t <- t(pc.makeup)

# Convert to data frame and keep variable names as column headers
pc.makeup.t <- as.data.frame(pc.makeup.t)
colnames(pc.makeup.t) <- rownames(pc.makeup)
rownames(pc.makeup.t) <- c("PC1", "PC2", "PC3")

pc.makeup.t %>%
  kable(
    digits = 3,
    caption = "Principal Component Loadings: First Three Components",
    booktabs = TRUE,
    align = "c"
  ) %>%
  kable_styling(
    latex_options = c("hold_position"),
    full_width = FALSE,
    position = "center"
  ) %>%
  footnote(
    general = "Loading contributions of chemical variables to the first three standardized principal components of the scaled log-transformed whisky trace element observations.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )

```



Within the PC space counterfeit samples are generally composed of positive PC2 scores (elevated levels of Br, Ca and S) and negative PC1 scores (reduced levels of the rest of the chemical trace elements). Inversely, most single-origin whiskies are characterized by positive PC1 scores (elevated levels of P, Cl, K, Mn, Fe, Cu, Zn) and negative PC2 scores (reduced levels of S, Br and S) with some variation. Blends and grains also are generally sit negatively below both the PC2 and PC1 abscissus and thus are primarily composed of low to average levels of most trace elements (Fig. \@ref(fig:biplot)).

```{r biplot, fig.cap="Biplot within the first two principal components feature space with chemical variable loadings, with data points colored by group (provenance, blend/grain and counterfeit) and shapes associated by class (counterfeit, grain, blend, highland, lowland, speyside, island). Confidence ellipses represent group distribution estimated within one standard deviation based on a multivariate normal distribution.", echo=FALSE}


logged.w.plot$sample <- as.factor(whisky_data$Sample_no)

s.logged.w.plot <- logged.w.plot

s.logged.w.plot[, 2:12] <- scale(s.logged.w.plot[, 2:12])

ggbiplot(PCA.whisky.log, obs.scale = 1, var.scale = 1,
         groups = s.logged.w.plot$Provenance,
         ellipse = TRUE,
         var.axes = TRUE) +
  geom_point(aes(shape = s.logged.w.plot$Descriptor,
                 color =s.logged.w.plot$Provenance), size = 4) +
  scale_shape_manual(values = c(15, 16, 17, 7, 8, 3, 4))  +
  geom_text(aes(label = logged.w.plot$sample),
            size = 3, vjust = -1, hjust = 0.5) +
  labs(color = "Whisky Group",
       shape = "Whisky Type") +
       guides(fill = "none") + 
  theme_tufte()
```




## K-means

With the stated starting parameter metrics, we produced k-means clustering algorithms using $k = 1,2,...,10$. We then extracted total within sum of squares (WSS) for each model, and via an elbow plot of WSS as a function of $k$, visually assessed a parsimonious $k^*$ candidate visually. The sharpest shift in proportional reduction of WSS occured at $k=3$, explaining 37.37% of all WSS composing the remaining variance reductions in $k = 2-10$ (Fig. \@ref(fig:scree)).  For $k=3$, 51.6% of variance is explained (Between SS/Total SS), average class silhouette width is 0.30, WSS = 164.9 and Between SS = 176.1. This shows moderate fit of clusters overall and in comparison to $k=4$ displays a fairly equitable fit, though cluster size ($k=3$: 10,6,16; $k=4$: 8,2,6,16) indicates more balanced clustering from $k=3$ modelling. $k=4$ isolates 2 points which appear highly differentiated in our feature space from all observations (Island 18, Island 19).

```{r scree, fig.cap="Elbow plot of k-means derived clusters for k = 1,2,..,10 using euclidian distance and the Hartigan-Wong algorithm. The dashed red line indicates likely optimal clustering at k=3, from visual assesment of proportion of variance reduced. Dark red shading indicates 37.37% of the variance explained from k=3 in comparison to every further group addition beyond k=3", echo=FALSE}

kmeans_scotch.func <- function(data){
  results <- list()
  for (i in 1:10){
    results[[as.character(i)]] <- kmeans(data,
                                         centers=i, iter.max = 100, nstart = 50)

  }

  wss <- sapply(results, function(x) sum(x$withinss))

  return(list(km_results = results, wss = wss))
}

scotch.kmean <- kmeans_scotch.func(scale.log.w[,-1])


wss.scotch <- data.frame(y = scotch.kmean$wss, x = c(1:10))



ggplot(wss.scotch, aes(x = x, y = y)) +
  geom_point(color = "black", size = 3) +
  geom_line(linetype = "solid", color = "black") +
  theme_tufte() +
  geom_vline(xintercept = 3, linetype = "dashed", color = "red", linewidth = 1) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 164.89678, ymax = 226.02211, alpha = 0.3, fill = "indianred") +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 62.03422, ymax = 164.89678, alpha = 0.2, fill = "coral") +
  theme_tufte() +
  scale_x_continuous(limits = c(1, 10), breaks = 1:10) +  # set ticks 1 to 10
  scale_y_continuous(limits = c(0, 350)) +
  labs(
    y = "Total Within Sum of Squares (WSS)",
    x = "Number of Clusters (K)"
  ) +
  theme(axis.text = element_text(angle =45, size = 10),
        axis.text.x = element_text(face = "italic"))

```

```{r kmeansum, echo =FALSE, warning=FALSE}

kmeans_summary_t <- data.frame(
  Metric = c("Cluster Sizes", "Variance Explained", "Avg Silhouette", 
             "Total Within SS", "Between SS", "Total SS"),
  K3 = c("10, 6, 16", "51.6%", "0.30", 
         round(scotch.kmean$km_results$`3`$tot.withinss, 2),
         round(scotch.kmean$km_results$`3`$betweenss, 2),
         round(scotch.kmean$km_results$`3`$totss, 2)),
  K4 = c("8, 2, 6, 16", "58.9%", "0.28",
         round(scotch.kmean$km_results$`4`$tot.withinss, 2),
         round(scotch.kmean$km_results$`4`$betweenss, 2),
         round(scotch.kmean$km_results$`4`$totss, 2))
)

kmeans_summary_t %>%
  kable(
    col.names = c("Metric", "K = 3", "K = 4"),
    caption = "K-means Clustering Comparison",
    booktabs = TRUE,
    align = c("l", "c", "c")
  ) %>%
  kable_styling(
    latex_options = "hold_position",
    full_width = FALSE,
    position = "center"
  ) %>%
  footnote(
    general = "Summary statistics for k-mean cluster analysis with k=3 and k=4.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )
```





Silhoutte plots (Fig. \@ref(fig:sil)) indicate that for $k=3$ clusters 2 ($S_2=0.36$) and 3 ($S_3=0.28$) have moderate fit, while cluster 1 ($S_1 = 0.15$) displays weak to insubstantial structure; only one observation (15 = young grain) displays a truly poor fit to a group ($S_i < 0$). Clustering suitability remains similar for groups 1 and 3 when $k = 4$, but the 2 points forming a new group indicate a very tight, suitable cluster ($S_4=0.49$; WSS = 140.08) while $S_2$ is reduced to 0.30. This trade-off only reduces average silhouette width by 0.02, indicating that there is minimal reduction in group fit and that this is a feasible cluster model as well. 

```{r sil, fig.cap="Silhouette plots for cluster candidates k=3 and k=4 displaying average silhoute scores. Silhouette scores range from -1 to 1, with suitable cluster structure indicated by higher positive scores.", echo=FALSE}

kmean.w.sil3 <- silhouette(scotch.kmean$km_results$`3`$cluster, dist(scale.log.w[,-1]))

kmean.w.sil4 <- silhouette(scotch.kmean$km_results$`4`$cluster, dist(scale.log.w[,-1]))

# Set up better plot dimensions and margins
par(mfrow = c(1,2),  # 1 row, 2 column
    cex.main = 1.2,
    cex.lab = 1.0,
    cex.axis = 0.9)


# Plot with better formatting

plot(kmean.w.sil3, 
     main = "", 
     border = NA,
     col = c("indianred", "skyblue", "lightgreen")[1:3],  
     cex.names = 0.8)      # Adjust cluster label size
mtext("a)", side = 3, line = 1, adj = 0, cex = 1.2, font = 2)

plot(kmean.w.sil4, 
     main = "", 
     border = NA,
     col = c("indianred", "skyblue", "lightgreen", "goldenrod")[1:4],
     cex.names = 0.8)
mtext("a)", side = 3, line = 1, adj = 0, cex = 1.2, font = 2)
```


The fourth group composed of 2 island observations ($k=4$) sits in the feature space characterized by both positive PC1 and PC2 scores, quite different from other group trends. This aligns with @shand2017multivariate's linear discriminant analysis (LDA) results utilizing the first three principal components, in which they were able to classify 2 out of 4 island whiskies correctly. For $k=3$ these observations are clustered along with 8 other single-origin whiskies, and all other cluster classifications remain the same. All five counterfeits have been correctly clustered, with one false positive which aligns with our poor fitting sample 15 (young grain). Further, all other grains and blends have been correctly classified into one group, but with 7 false positives of provenance whiskies also classified into that group.

```{r kmeanbi, fig.cap="Biplots of data-points within principal components feature space by k-means clusters with ellipses indicating a confidence interval within one standard error. Legend denotes original whisky class. (a) Displays k=3 model with cluster 1 indicated by red, cluster 2 by green, and cluster 3 by blue colouration. (b) Displays k=4 model with cluster 1 indicated by red, cluster 2 green, by cluster 3 by aqua, and cluster 4 by purple coloration.", warning=FALSE, echo=FALSE, }


suppressWarnings({
kmean3 <- ggbiplot(PCA.whisky.log, obs.scale = 1, var.scale = 1,
         groups = as.factor(scotch.kmean$km_results$`3`$cluster), ellipse = TRUE) +
  geom_point(aes(shape = logged.w.plot$Descriptor,
                 color = as.factor(scotch.kmean$km_results$`3`$cluster)), size = 4)  +
  scale_shape_manual(values = c(15, 16, 17, 7, 8, 3, 4), name = "Descriptor") +
  guides(color = "none", fill = "none") +  # Remove color legend
  geom_text(aes(label = logged.w.plot$sample),
            size = 3, vjust = -1, hjust = 0.5) +
  theme_tufte() +
  labs(tag = "(a)", title = "K = 3")

kmean4 <- ggbiplot(PCA.whisky.log, obs.scale = 1, var.scale = 1,
         groups = as.factor(scotch.kmean$km_results$`4`$cluster), ellipse = TRUE) +
  geom_point(aes(shape = logged.w.plot$Descriptor,
                 color = as.factor(scotch.kmean$km_results$`4`$cluster)), size = 4)  +
  scale_shape_manual(values = c(15, 16, 17, 7, 8, 3, 4), name = "Descriptor") +
  guides(color = "none", fill = "none") +  # Remove color legend
  geom_text(aes(label = logged.w.plot$sample),
            size = 3, vjust = -1, hjust = 0.5) +
  theme_tufte() +
  labs(tag = "(b)", title = "K = 4")
})

# Combine with shared shape legend only
comb.plot <-kmean3 + kmean4 + 
  plot_layout(guides = "collect") &
  theme(plot.tag = element_text(face = "bold", size = 14),
        legend.position = "bottom")




print(comb.plot)

```


## PAM

Overall, PAM clustering at $k=3$ produced fairly equitable results to k-means clustering at the same cluster size, but at $k=4$ produced much different results (Table \@ref(tab:pamsum)). At $k=3$ the average silhouette width was 0.294 with relatively similar by-group $s_i$, and group sizes of similar observations with medoids 4 (Blend), 10 (counterfeit) and 18 (island). However, for $k=4$ the average silhouette width decreased to 0.149, with group 1 (Fig. \@ref(fig:pambi)) being separated into two which drastically reduced that clusters separation metric from 2.304 to 1.839 per cluster, heavily reducing cluster $s_i$ for that group as well as for the cluster primarily composed of counterfeits (Table \@ref(tab:pamsum)). Due to both the reduction in silhouette fit and inconsistancies with k-means modelling at $k=4$, we will distinguish $k^*=3$ as our robust and optimal group number.

For $k^*$ clustering results were almost identical to k-means, except counterfeits were isolated completely with no false positives and the young grain observation (15) was included in cluster 1 along with the other 9 previously grouped grains and blends, and the 7 provenance whiskies. 

```{r pamsum, echo=FALSE, warning=FALSE}

kmedoid_scotch.func <- function(data, k_max = 10) {
  pam_fit <- list()
  wss <- numeric(k_max)        # preallocate
  sil_width <- numeric(k_max)  # preallocate

  for (i in 2:k_max) { # PAM doesn't work with k=1
    pam_fit[[as.character(i)]] <- pam(data, k = i, metric = "euclidean")

    # within-cluster dissimilarity (objective function)
    wss[i] <- pam_fit[[as.character(i)]]$objective[2]

    # average silhouette width
    sil_width[i] <- pam_fit[[as.character(i)]]$silinfo$avg.width
  }

  return(list(pam_fit = pam_fit, wss = wss, silhouette = sil_width))
}

pam.scotch <- kmedoid_scotch.func(scale.log.w[,-1])


pam3_summary <- data.frame(
  Cluster = 1:3,
  Size = pam.scotch$pam_fit$`3`$clusinfo[, "size"],
  Medoid_ID = pam.scotch$pam_fit$`3`$id.med,
  Avg_Dissimilarity = round(pam.scotch$pam_fit$`3`$clusinfo[, "av_diss"], 3),
  Separation = round(pam.scotch$pam_fit$`3`$clusinfo[, "separation"], 3),
  Avg_Silhouette = round(pam.scotch$pam_fit$`3`$silinfo$clus.avg.widths, 3)
)




pam4_summary <- data.frame(
  Cluster = 1:4,
  Size = pam.scotch$pam_fit$`4`$clusinfo[, "size"],
  Medoid_ID = pam.scotch$pam_fit$`4`$id.med,  Avg_Dissimilarity = round(pam.scotch$pam_fit$`4`$clusinfo[, "av_diss"], 3),
  Separation = round(pam.scotch$pam_fit$`4`$clusinfo[, "separation"], 3),
  Avg_Silhouette = round(pam.scotch$pam_fit$`4`$silinfo$clus.avg.widths, 3)
)

# Combine the summaries with a K column
pam3_summary$K <- 3
pam4_summary$K <- 4

combined_summary <- rbind(pam3_summary, pam4_summary)

# Get overall silhouettes for footer
overall_sil_3 <- round(pam.scotch$pam_fit$`3`$silinfo$avg.width, 3)
overall_sil_4 <- round(pam.scotch$pam_fit$`4`$silinfo$avg.width, 3)

combined_summary %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    caption = "PAM Clustering Results",
    col.names = c("K", "Cluster", "Size", "Medoid", "Avg Diss.", 
                  "Separation", "Avg Silhouette")
  ) %>%
  kable_styling(latex_options = c("hold_position")) %>%
  footnote(
    general = paste0("Summary statistics for PAM cluster analysis with k=3 and k=4. ",
                     "K=3 Overall Avg Silhouette: ", overall_sil_3, 
                     "; K=4 Overall Avg Silhouette: ", overall_sil_4, "."),
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )


```

```{r pambi, fig.cap="Biplots of data-points within principal components feature space by PAM clustering with ellipses indicating a confidence interval within one standard error. Legend denotes original whisky class. (a) Displays k=3 model with cluster 1 indicated by red, cluster 2 by green, and cluster 3 by blue colouration. (b) Displays k=4 model with cluster 1 indicated by red, cluster 2 green, by cluster 3 by aqua, and cluster 4 by purple coloration.", warning=FALSE, echo=FALSE}

pam3plot <- ggbiplot(PCA.whisky.log, obs.scale = 1, var.scale = 1,
         groups = as.factor(pam.scotch$pam_fit$`3`$clustering), ellipse = TRUE) +
geom_point(aes(shape = logged.w.plot$Descriptor,
               color = as.factor(pam.scotch$pam_fit$`3`$clustering)), size = 4)  +
  scale_shape_manual(values = c(15, 16, 17, 7, 8, 3, 4), name = "Descriptor") +
  guides(color = "none", fill = "none") +  # Remove color legend
  geom_text(aes(label = logged.w.plot$sample),
            size = 3, vjust = -1, hjust = 0.5) +
  theme_tufte()

pam4plot <- ggbiplot(PCA.whisky.log, obs.scale = 1, var.scale = 1,
         groups = as.factor(pam.scotch$pam_fit$`4`$clustering), ellipse = TRUE) +
geom_point(aes(shape = logged.w.plot$Descriptor,
               color = as.factor(pam.scotch$pam_fit$`4`$clustering)), size = 4)  +
  scale_shape_manual(values = c(15, 16, 17, 7, 8, 3, 4), name = "Descriptor") +
  guides(color = "none", fill = "none") +  # Remove color legend
  geom_text(aes(label = logged.w.plot$sample),
            size = 3, vjust = -1, hjust = 0.5) +
  theme_tufte()

# Combine with shared shape legend only
comb.plot2 <- pam3plot + pam4plot + 
  plot_layout(guides = "collect") +
  plot_annotation(tag_levels = 'a') &
  theme(plot.tag = element_text(face = "bold", size = 14),
        legend.position = "bottom")
print(comb.plot2)
  
```




## Hierarchical clustering

Using $k^*$, we reproduced dendrogram results from @shand2017multivariate using agglomerative hierarchical clustering with euclidean point-to-point distance and complete linkage, but could find no other distance and linkage combination which were consistent with these results (Fig. \@ref(fig:ahclust)). These included implementing various cases of power distances emphasizing large differences in our feature space such as the Chebyshev (linkage: complete and Ward) and the Minkowski (a=3; linkage: complete and Ward), as the model from @shand2017multivariate separated those observations belonging to more distinctly different regions of our feature space (observations 18:island, 19:island, and counterfeit whiskies) while retaining a third cluster of fairly homogeneous observations across blends and provenance whiskies.

As we found these results relatively non-useful, we proposed finding distance and linkage combinations which would reinforce our previous k-means and PAM clustering results at $k^*$. We found the using the euclidian (Ward linkage) and the Manhattan (both complete and Ward) produced equal cluster results to k-means clustering at $k^*$. Further we applied a Pearson correlation coefficient distance ($d=1-r$) between whisky observations combined with a Ward linkage, such that whiskies should be aggregated by similar relative patterns of chemical variables, even if absolute concentrations differ. We thought this method of profiling whiskies this way may yield interesting results, and clustered counterfeits as k-means clustering did but only classified 7 rather than 10 provenance whiskies into a single group, adding the remaining three to a larger pool of blend, grain and provenance whiskies (Fig. \@ref(fig:ahclust)). 


```{r ahclust, fig.cap="Agglomerative hierarchical clustering plots for whisky data. Panel (a) denotes the repordiced figure from Shand et al., 2017 using euclidean distance with a complete linkage; (b) denotes the euclidean distance using Ward linkage; (c) denotes the manhattan distance using complete linkage; (d) denotes the correlation distance (1-r) using Ward Ward linkage. Coloured boxes segregate selected k=3 groups given the distance and linkage.", echo=FALSE, warning = FALSE, fig.height=9, fig.width=11}


pc_scores.v <- as.data.frame(PCA.whisky.log$x[,1:2]) 

pc_scores.v$ID <- whisky_data$Sample_no

pc_scores.v <- as.data.frame(PCA.whisky.log$x[, 1:2])
pc_scores.v$Descriptor <- logged.c.whisky$Descriptor
pc_scores.v$Label <- paste(1:32, pc_scores.v$Descriptor, sep = "-")


dist_matrix <- dist(scale(logged.c.whisky[, -1]), method = "euclidean")
hc_whisky <- hclust(dist_matrix, method = "complete")


hc_whisky.w <- hclust(dist_matrix, method = "ward.D2")

dist_matrix4 <- dist(scale(logged.c.whisky[, -1]), method = "manhattan")


hc_whisky4 <- hclust(dist_matrix4, method = "complete")

dist_matrix3 <- as.dist(1 - cor(t(logged.c.whisky[-1])))


hc_whisky3 <- hclust(dist_matrix3, method = "ward.D2")




par(mfrow = c(2, 2), 
    mar = c(5, 4, 4, 2),
    oma = c(0, 0, 0, 0))

# Plot 1: Euclidean Complete Linkage
plot(hc_whisky,
     main = "",
     xlab = "Whisky Index",
     ylab = "Height",
     hang = -1,
     labels = pc_scores.v$Label)
rect.hclust(hc_whisky, k = 3, border = c("indianred", "orange", "skyblue"))
title(main = "Euclidean - Complete Linkage", line = 2)
mtext("(a)", side = 3, line = 3, at = par("usr")[1], adj = 0, font = 2)

# Plot 2: Euclidean Ward's Method
plot(hc_whisky.w,
     main = "",
     xlab = "Whisky Index",
     ylab = "Height",
     hang = -1,
     labels = pc_scores.v$Label)
rect.hclust(hc_whisky.w, k = 3, border = c("indianred", "orange", "skyblue"))
title(main = "Euclidean - Ward's Linkage", line = 2)
mtext("(b)", side = 3, line = 3, at = par("usr")[1], adj = 0, font = 2)

# Plot 3: Manhattan Complete Linkage
plot(hc_whisky4,
     main = "",
     xlab = "Whisky Index",
     ylab = "Height",
     hang = -1,
     labels = pc_scores.v$Label)
rect.hclust(hc_whisky4, k = 3, border = c("indianred", "orange", "skyblue"))
title(main = "Manhattan - Complete Linkage", line = 2)
mtext("(c)", side = 3, line = 3, at = par("usr")[1], adj = 0, font = 2)

# Plot 4: Correlation Ward's Method
plot(hc_whisky3,
     main = "",
     xlab = "Whisky Index",
     ylab = "Height",
     hang = -1,
     labels = pc_scores.v$Label)
rect.hclust(hc_whisky3, k = 3, border = c("indianred", "orange", "skyblue"))
title(main = "Correlation (1-r) - Ward's Method", line = 2)
mtext("(d)", side = 3, line = 3, at = par("usr")[1], adj = 0, font = 2)

# Reset to default single plot layout
par(mfrow = c(1, 1))
```



## Quality metrics

Confusion matrices were calculated, and global and classwise quality metrics derived. K-means (at $k^*$), Manhattan agglomerative (complete and Ward), and Euclidian agglomerative (Ward) all displayed equal clustering results. Global quality statistics displayed that PAM clustering consistantly performed the best with the highest metrics: $OAcc$ (78.1%), $AAcc$ (0.854), $F1\text{-score}_M$ (82.7%), $TNR_M$ (89.4%), $F1\text{-score}_{\mu}$ (79.3%), $PPV_{\mu}$ (83.5%), and $TPR_{\mu}$ (79.3%). K-means, euclidian (Ward) agglomerative  and Manhattan (complete and Ward) agglomeration modelling displayed similar performance ($OAcc = 0.75$; Table \@ref(tab:globmetric), followed by poorer performance by correlation distance agglomeritive clustering. Euclidean (complete) clustering performed the poorest overall across all metrics (Table \@ref(tab:globmetric).

```{r matfunc, echo=FALSE, warning=FALSE}
#matrix function
confusion_metric <- function(c_matrix) {


  ################## produce class measures of quality ##########################

  ###### class TP,FP,TN, FN counts

  # generate an empty list for TP, FP, TN, FN counts
  rawposneg <- list()


  #1: true positives per class
  rawposneg$TP <- diag(c_matrix)


  #2: true negatives per class


  rawposneg$TN <- sum(c_matrix) - rowSums(c_matrix) - colSums(c_matrix) + rawposneg$TP

  #3: False positives per class

  #vector generated as before
  rawposneg$FP <-  rowSums(c_matrix) - diag(c_matrix)


  #4: False negative per class

  rawposneg$FN <- colSums(c_matrix) - diag(c_matrix)

  # convert list to table for easier viewing

  #determine number of classes for table output and further quality metrics
  n <- ncol(c_matrix)

  count_table <- as.data.frame(rawposneg)
  rownames(count_table) <- paste0("Class_", 1:n)

  ###### class quality metrics


  #extract TP,TN,FP and FN from list structure
  TP <- rawposneg$TP
  FP <- rawposneg$FP
  FN <- rawposneg$FN
  TN <- rawposneg$TN


  classquality_meas <- list()

  #1: Accuracy - ACC_i
  classquality_meas$ACC_i <- (TP + TN)/ (TP + TN + FP + FN)

  #2: Misclassification rate - MR_i
  classquality_meas$MR_i <- (FP + FN)/ (TP + TN + FP + FN)

  #3: Precision - PPV_i
  classquality_meas$PPV_i <- TP/ (TP + FP)

  #4: Recall - TPR_i
  classquality_meas$TPR_i <- TP/ (TP + FN)

  #5: Specificity - TNR_i
  classquality_meas$TNR_i <- TN/ (TN + FP)

  #6: F1-score_i
  PPV_i <- classquality_meas$PPV_i
  TPR_i <- classquality_meas$TPR_i
  classquality_meas$F_class <- (2*PPV_i*TPR_i)/(PPV_i + TPR_i)


  classqual_table <- as.data.frame(classquality_meas)
  rownames(classqual_table) <- paste0("Class_", 1:n)

  ######################## global quality measures ##########################

  #generate an empty list for global quality metrics
  globalquality_meas <- list()

  #1: OAcc or overall accuracy
  globalquality_meas$OAcc <- sum(diag(c_matrix))/sum(c_matrix)

  ###### averaged metrics

  #2: Average accuracy or AAcc
  globalquality_meas$AAcc <- sum(classquality_meas$ACC_i)/n

  #3: F1-score_m
  globalquality_meas$F1_m <- sum(classquality_meas$F_class)/n

  #4:Macro Precision or TNR_m
  globalquality_meas$TNR_m <- sum(classquality_meas$TNR_i)/n

  ###### weighted metrics

  #1: Micro F-1 score - F1_mu

  globalquality_meas$F1_mu <- (sum(TP * classquality_meas$F_class))/(sum(TP))

  #2: Micro Precision - PPV_mu

  globalquality_meas$PPV_mu <- (sum(TP * PPV_i))/(sum(TP))

  #3:Micro Recall - TPR_mu

  globalquality_meas$TPR_mu <- (sum(TP * TPR_i))/(sum(TP))

  globalqual_table <- as.data.frame(globalquality_meas)


  #Return TRUE if each condition held to allow to see where matrix fails
  #Return both class and global quality metrics
  return(list(count_table =  count_table,
              classqual_table = classqual_table,
              globalqual_table = globalqual_table))
}

```



```{r processmat, echo=FALSE, warning=FALSE}


clusters_hc3euc <- cutree(hc_whisky, k = 3)
pc_scores.v$cluster_hc3euc <- clusters_hc3euc

clusters_hc3euc.w <- cutree(hc_whisky.w, k = 3)
pc_scores.v$cluster_hc3euc.w <- clusters_hc3euc.w

clusters_hc3manh <- cutree(hc_whisky4, k = 3)
pc_scores.v$cluster_hc3manh <- clusters_hc3manh

clusters_hc3cor <- cutree(hc_whisky3, k = 3)
pc_scores.v$cluster_hc3cor <- clusters_hc3cor

#kmean

logged.w.plot$kmean <- scotch.kmean$km_results$`3`$cluster

logged.w.plot$kmean <- factor(logged.w.plot$kmean,
                              levels = c(2, 3, 1),
                              labels = c("Counterfeit", "Grain_Blend","Provenance"))

kmean_c_matrix <- table(Predicted = logged.w.plot$kmean,
                          Actual = logged.w.plot$Provenance)




logged.w.plot$pam <- pam.scotch$pam_fit$`3`$clustering


logged.w.plot$pam <- factor(logged.w.plot$pam,
                              levels = c(2, 1, 3),
                              labels = c("Counterfeit", "Grain_Blend", "Provenance"))

pam_c_matrix <- table(Predicted = logged.w.plot$pam,
                        Actual = logged.w.plot$Provenance)



#manh, kmean, euc (ward)


logged.w.plot$manh <- clusters_hc3manh

logged.w.plot$manh <- factor(logged.w.plot$manh,
                            levels = c(2, 1, 3),
                            labels = c("Counterfeit", "Grain_Blend", "Provenance"))

manh_c_matrix <- table(Predicted = logged.w.plot$manh,
                      Actual = logged.w.plot$Provenance)







logged.w.plot$euc <- clusters_hc3euc




logged.w.plot$euc <- factor(logged.w.plot$euc,
                             levels = c(2, 1, 3),
                             labels = c("Counterfeit", "Grain_Blend", "Provenance"))

euc_c_matrix <- table(Predicted = logged.w.plot$euc,
                       Actual = logged.w.plot$Provenance)




logged.w.plot$cor <- clusters_hc3cor 

logged.w.plot$cor <- factor(logged.w.plot$cor,
                            levels = c(2, 1, 3),
                            labels = c("Counterfeit", "Grain_Blend", "Provenance"))

corr_c_matrix <- table(Predicted = logged.w.plot$cor,
                      Actual = logged.w.plot$Provenance)


```



```{r globmetric, echo=FALSE, warning=FALSE}




#kmean

logged.w.plot$kmean <- scotch.kmean$km_results$`3`$cluster

logged.w.plot$kmean <- factor(logged.w.plot$kmean,
                              levels = c(2, 3, 1),
                              labels = c("Counterfeit", "Grain_Blend","Provenance"))

kmean_c_matrix <- table(Predicted = logged.w.plot$kmean,
                          Actual = logged.w.plot$Provenance)




logged.w.plot$pam <- pam.scotch$pam_fit$`3`$clustering


logged.w.plot$pam <- factor(logged.w.plot$pam,
                              levels = c(2, 1, 3),
                              labels = c("Counterfeit", "Grain_Blend", "Provenance"))

pam_c_matrix <- table(Predicted = logged.w.plot$pam,
                        Actual = logged.w.plot$Provenance)



#manh, kmean, euc (ward)


logged.w.plot$manh <- clusters_hc3manh

logged.w.plot$manh <- factor(logged.w.plot$manh,
                            levels = c(2, 1, 3),
                            labels = c("Counterfeit", "Grain_Blend", "Provenance"))

manh_c_matrix <- table(Predicted = logged.w.plot$manh,
                      Actual = logged.w.plot$Provenance)







logged.w.plot$euc <- clusters_hc3euc




logged.w.plot$euc <- factor(logged.w.plot$euc,
                             levels = c(2, 1, 3),
                             labels = c("Counterfeit", "Grain_Blend", "Provenance"))

euc_c_matrix <- table(Predicted = logged.w.plot$euc,
                       Actual = logged.w.plot$Provenance)




logged.w.plot$cor <- clusters_hc3cor 

logged.w.plot$cor <- factor(logged.w.plot$cor,
                            levels = c(2, 1, 3),
                            labels = c("Counterfeit", "Grain_Blend", "Provenance"))

corr_c_matrix <- table(Predicted = logged.w.plot$cor,
                      Actual = logged.w.plot$Provenance)


cons.metric <- confusion_metric(manh_c_matrix)
PAMmetric <- confusion_metric(pam_c_matrix)
cor.metric <- confusion_metric(corr_c_matrix)
eucmetric <- confusion_metric(euc_c_matrix)

clustering_comparison <- data.frame(
  Method = c("Consensus", "PAM", "Correlation HC", "Euclidean HC"),
  Overall_Accuracy = c(cons.metric$globalqual_table$OAcc,
                       PAMmetric$globalqual_table$OAcc,
                       cor.metric$globalqual_table$OAcc,
                       eucmetric$globalqual_table$OAcc),
  Average_Accuracy = c(cons.metric$globalqual_table$AAcc,
                       PAMmetric$globalqual_table$AAcc,
                       cor.metric$globalqual_table$AAcc,
                       eucmetric$globalqual_table$AAcc),
  F1_Macro = c(cons.metric$globalqual_table$F1_m,
               PAMmetric$globalqual_table$F1_m,
               cor.metric$globalqual_table$F1_m,
               eucmetric$globalqual_table$F1_m),
  TNR_Macro = c(cons.metric$globalqual_table$TNR_m,
                PAMmetric$globalqual_table$TNR_m,
                cor.metric$globalqual_table$TNR_m,
                eucmetric$globalqual_table$TNR_m),
  F1_Micro = c(cons.metric$globalqual_table$F1_mu,
               PAMmetric$globalqual_table$F1_mu,
               cor.metric$globalqual_table$F1_mu,
               eucmetric$globalqual_table$F1_mu),
  TNR_Micro = c(cons.metric$globalqual_table$PPV_mu,
               PAMmetric$globalqual_table$PPV_mu,
               cor.metric$globalqual_table$PPV_mu,
               eucmetric$globalqual_table$PPV_mu),
   TPR_Micro = c(cons.metric$globalqual_table$F1_mu,
               PAMmetric$globalqual_table$F1_mu,
               cor.metric$globalqual_table$F1_mu,
               eucmetric$globalqual_table$F1_mu)
)

clustering_comparison %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    caption = "Clustering Method Performance Comparison",
    col.names = c("Method", "Overall Acc.", "Average Acc.", "F1 (Macro)", 
                  "TNR (Macro)", "F1 (Micro)", "TNR (Micro)", "TPR (Micro)"),
    digits = 3,
    escape = FALSE
  ) %>%
  kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  row_spec(which.max(clustering_comparison$Overall_Accuracy), 
           bold = TRUE, background = "lightgray") %>%
  footnote(
    general = "Global confusion quality metrics for evaluating clustering method performance at k =3; the best performing model is highlighted with grey background. Consensus results refer to aggreeing clustering models (k-means, hierarchical (euclidean [Ward], manhattan [Ward & complete]))",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )








```




Pam class-wise performance shows that this model classified all counterfeits correctly, aligning with @shand2017multivariate's LDA results (Table \@ref(tab:classqual). However, our provenance class displayed a specificity of 68.2% ($TNR_i$) and a 58.8% recall ($TPR_i$) showing that approximately 41% of this class were misclassified as blended whiskies (false negatives). This, along with perfect precision and our weaker recall shows that a very strong true positive signal, but moderately strong chance of incurring false negatives. Inversely, our presision ($PPV_i$) for blended and grains was 58.8%, showing that approximately 41% of predictions within this class were wrong (false positives), with our specificity ($TNR_i$) of 68.2% indicating a moderate lack of class differentiation ability between blended/grain and provenance whiskies .Both classes sharing a balanced accuracy ($Acc_i$) of 78.1%, though their deficits and strengths differ.

```{r classqual, echo=FALSE, warning=FALSE}
rownames(PAMmetric$classqual_table) <- c("Counterfeit", "Blended", "Provenance")

combined_tab <- cbind(PAMmetric$count_table,
                      PAMmetric$classqual_table)
rownames(combined_tab) <- c("Counterfeit", "Blended", "Provenance")

combined_tab %>%
  rownames_to_column("Class") %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    caption = "PAM Class-wise Performance",
    digits = 3,
    escape = TRUE  # Changed from FALSE to TRUE
  ) %>%
  kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  row_spec(which.max(combined_tab$F_class), 
           bold = TRUE, background = "lightgray") %>%
  footnote(
    general = "Counts and derived quality metrics for PAM clustering performance by class, perfect classification is highlighted in grey.",
    general_title = "Note:",
    footnote_as_chunk = TRUE,
    threeparttable = TRUE
  )

```

# Discussion

Through multivariate analysis of the trace element measurements of Scotch whisky using TXRF, we have found it to be a sound 
method for differentiating between counterfeits, blends and grains, and regional provenance whiskies. 

We found LDA - as used in @shand2017multivariate - to be suitable for counterfeit detection in situations where lower fidelity is desired, 
or for initial data exploration, but found it inappropriate to use for strong conclusions due to the covariance matrix heterogeneity introduced 
by the small proportion of counterfeit samples in the data, and the overall low number of samples. 

We examined multiple methods of cluster analysis, finding PAM clustering at k=3 to be the most successful at separating out counterfeits 
from other whisky groups, while Euclidean distance with complete linkage used in @shand2017multivariate provides less reliable results. 
We found using ward linkage provides results more aligned with our best performing models. The variation in clustering results suggests 
that care should be taken when selecting appropriate distances and number of clusters, best seen in the hierarchical clustering.

No method of clustering analysis incorrectly predicted a provenance whisky to be counterfeit, or a counterfeit to be of provenance, 
indicating the success of this method for identifying counterfeits. The PAM clustering method identified counterfeits with 100% accuracy, 
with mild confusion between blend/grain and provenance whiskies. The lower success of other clustering methods, identifying some blend/grains as counterfeits, 
suggests that these counterfeits could be partly composed of these types of whiskies and various additives (@power2020brief). 
This could further suggest that counterfeits being passed as blend/grains may be more likely to go undetected, compared to being passed as regional provenance whiskies.

The moderate to weak clustering performance observed when distinguishing between blend/grain whiskies, and regional provenance whiskies would indicate that more or 
different data would be required in order to more confidently make a distinction between these two groups, including the age of the whisky, and the aging process 
used in production, as these would introduce different metals and compounds, and affect the trace element composition of whisky samples.

Our success in detecting counterfeit whisky using TXRF, combined with its affordability and portability, 
leads us to recommend it as a valid means of counterfeit detection in field applications, and would encourage further research into its use. 


# Reference
